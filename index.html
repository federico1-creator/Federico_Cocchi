<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Federico Cocchi</title>

    <meta name="author" content="Federico Cocchi">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="personal_data\image.jpg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Federico Cocchi
                </p>
                <p>I'm a Italian National PhD student in Artificial Intelligence at <a href="https://aimagelab.ing.unimore.it/imagelab/">AImageLab</a>.
                  My research activities involve Multimodal Large Language Models, Large Language Models and Deepfake detection.
                  My interests extend to Generative AI and HPC systems. 
                  <!-- I actively leverage AI technologies in my daily activities to introduce breakthroughs and resolve in an innovative and disruptive way new and old problems. -->
                </p>
                <p>
                  I had the opportunity to touch for the first time the world of AI, during my Bachelor's thesis. 
                  Specifically, in the field of Anomaly Detection in HPC systems. During this time I expanded a framework designed 
                  to prevent potential failures in HPC clusters. This work was possible thanks to the collaboration with 
                  my supervisors <a href="https://scholar.google.com/citations?user=bQBpCnEAAAAJ&hl=it">Prof. Michela Milano</a> 
                  and <a href="https://scholar.google.com/citations?hl=en&user=8riq3sYAAAAJ&view_op=list_works&sortby=pubdate">Prof. Luca Benini</a>.
                </p>
                <p>
                  My initial engagement with AImageLab began during my Master’s thesis, where I worked on the problem of 
                  Deepfake detection under the supervision of <a href="https://scholar.google.com/citations?user=OM3sZEoAAAAJ&hl=it">Prof. Rita Cucchiara</a>, 
                  contributing to the development of datasets and models publicly released under the European project <a href="https://www.elsa-ai.eu/">ELSA</a>.
                </p>

                <p>
                  <ul>
                    <li><a href="https://github.com/aimagelab/LLaVA-MORE">LLaVA-MORE:</a> Enhancing Visual Instruction Tuning with LLaMA 3.1</li>
                    <li><a href="https://huggingface.co/collections/fede97/latingpt-6687d2eb4f666aaf1029a679">LatinGPT:</a> first generative model trained from scratch with Latin data</li>
                    <li>[ECCV 2024 DEMO]: Showcase Contrasting Deepfakes Embeddings</li>
                  </ul>
                </p>
                
                <!-- INFO ELEMENTS -->
                <p style="text-align:center">
                  <a href="mailto:federico.cocchi@unimore.it">Email</a> &nbsp;/&nbsp;
                  <a href="personal_data/CV_FC.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="personal_data/my-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <!-- <a href="https://scholar.google.com/citations?user=BRG3e1EAAAAJ&hl=it">Scholar</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=BRG3e1EAAAAJ&hl=it" target="_blank"> 
                    <img src="image\logo\google-scholar-icon.png" alt="GitHub" style="width:12px;height:12px;"> Scholar
                  </a> &nbsp;/&nbsp;
                  <a href="https://huggingface.co/fede97" target="_blank">
                    <img src="image\logo\hf_logo.png" alt="GitHub" style="width:12px;height:12px;"> HuggingFace
                  </a> &nbsp;/&nbsp;
                  <a href="https://github.com/federico1-creator" target="_blank">
                    <img src="image\logo\GitHub-logo.png" alt="GitHub" style="width:12px;height:12px;"> GitHub
                  </a>
                </p>

              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="personal_data/image.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="personal_data/image.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am interested in Generative AI, Computer Vision and Natural Language Processing. 
                  Most of my research explores the intersection of these fields. 
                  Moreover, below is a representative list of my research papers.
                  <!-- Representative papers are <span class="highlight">highlighted</span>. -->
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="image\model_reflectiva.png" alt="code" width="160" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2411.16863"> 
              <span class="papertitle">Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering</span>
            </a>
            <br>
            <strong>Federico Cocchi*</strong>,
            <a>Nicholas Moratelli*</a>,
            <a>Marcella Cornia</a>,
            <a>Lorenzo Baraldi</a>,
            <a>Rita Cucchiara</a>,
            <br>
            <em>IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2025
            <br>
            <a href="https://arxiv.org/pdf/2411.16863">paper</a> /  
            <a href="references/reflectiva.bib">bibtex</a>
            <p>Reflective LLaVA (ReflectiVA) enhances Multimodal LLMs by integrating external knowledge, 
              using reflective tokens to determine and retrieve relevant information dynamically. 
              This approach improves knowledge-based visual question answering, outperforming existing methods while 
              maintaining fluency and performance on standard Multimodal benchmarks.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="image/survey.png" alt="code" width="160" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2402.12451"> 
              <!-- TODO: add link to the paper -->
              <span class="papertitle">The (R)Evolution of Multimodal Large Language Models: A Survey</span>
            </a>
            <br>
            <a>Davide Caffagni*</a>,
            <strong>Federico Cocchi*</strong>,
            <a>Luca Barsellotti*</a>,
            <a>Nicholas Moratelli*</a>,
            <a>Sara Sarto*</a>,
            <a>Lorenzo Baraldi*</a>,
            <a>Lorenzo Baraldi</a>,
            <a>Marcella Cornia</a>,
            <a>Rita Cucchiara</a>,
            <br>
            <em>Annual Meeting of the Association for Computational Linguistics (ACL) Findings</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2402.12451">paper</a> /
            <a href="poster/2024_Poster_survey_multimodal.pdf">poster</a> /
            <a href="references/survey_mllm.bib">bibtex</a>
            <p>In this paper we introduce the emergence of Multimodal Large Language Models (MLLMs), highlighting their ability to seamlessly integrate both textual and visual modalities.
              Exploring architectural choices, alignment strategies, and training techniques employed in MLLMs.
              Additionally, the paper provides insights into the performance and computational requirements of existing models across tasks such as visual grounding, image generation, 
              and domain-specific applications, serving as a foundational resource for future advancements in MLLMs.</p>
          </td>
        </tr>
    
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="image/wiki_llava.png" alt="code" width="160" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://scholar.google.com/citations?view_op=view_citation&hl=it&user=BRG3e1EAAAAJ&citation_for_view=BRG3e1EAAAAJ:qjMakFHDy7sC"> 
              <span class="papertitle">Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs</span>
            </a>
            <br>
            <a>Davide Caffagni*</a>,
            <strong>Federico Cocchi*</strong>,
            <a>Nicholas Moratelli*</a>,
            <a>Sara Sarto*</a>,
            <a>Marcella Cornia</a>,
            <a>Lorenzo Baraldi</a>,
            <a>Rita Cucchiara</a>,
            <br>
            <em> IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) Workshop</em>, 2024
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2024W/MMFM/papers/Caffagni_Wiki-LLaVA_Hierarchical_Retrieval-Augmented_Generation_for_Multimodal_LLMs_CVPRW_2024_paper.pdf">paper</a> /
            <a href="poster/2024_Poster_wiki-llava.pdf">poster</a> /
            <a href="references/wiki_llava.bib">bibtex</a>
            <br>
            <p>Multimodal LLMs (MLLMs) extend the capabilities of LLMs beyond textual modalities. We put our focus on enhancing these models to answer questions that required external knowledge. 
              The proposed approach, Wiki-LLaVA, integrates an external knowledge source via a hierarchical retrieval pipeline. Extracted relevant passages augment the LLM's context, 
              enhancing the precision of generated dialogues. Extensive experiments on question-answering datasets validate the effectiveness of the approach without losing performance in standard
              multimodal benchmarks (MMMU, POPE, MME, MMB).</p>
          </td>
        </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="image/safe_clip_model-1.png" alt="safe-clip" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2311.16254">
          <span class="papertitle">Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models</span>
        </a>
        <br>
        <a>Samuele Poppi*</a>,
        <a>Tobia Poppi*</a>,
        <strong>Federico Cocchi*</strong>,
        <a>Marcella Cornia</a>,
        <a>Lorenzo Baraldi</a>,
        <a>Rita Cucchiara</a>,
        <br>
        <em>European Conference on Computer Vision (ECCV)</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2311.16254">paper</a> /  
        <a href="https://github.com/aimagelab/safe-clip">code</a> /
        <a href="https://aimagelab.github.io/safe-clip/">project page</a> /
        <a href="https://huggingface.co/collections/aimagelab/safe-clip-668d0a0ca697b69d66433338">model</a> /
        <a href="https://huggingface.co/datasets/aimagelab/ViSU-Text">dataset</a> /
        <a href="references/safe_clip.bib">bibtex</a>
        <p>This paper improves the safety
           of Vision-and-Language models like CLIP by reducing sensitivity to NSFW content.
          Showing results on Image generation and Retrieval.
          <!--both Image to Text and Text to Image-->
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="image/deepfake_model-1.png" alt="code" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2407.20337"> 
          <!-- TODO: add link to the paper -->
          <span class="papertitle">Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities</span>
        </a>
        <br>
        <a>Lorenzo Baraldi*</a>,
        <strong>Federico Cocchi*</strong>,
        <a>Marcella Cornia</a>,
        <a>Lorenzo Baraldi</a>,
        <a>Alessandro Nicolosi</a>,
        <a>Rita Cucchiara</a>,
        <br>
        <!-- <em>arxiv</em>, 2023 -->
        <em>European Conference on Computer Vision (ECCV)</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2407.20337">paper</a> /  
        <a href="https://github.com/aimagelab/CoDE">code</a> /
        <a href="https://aimagelab.github.io/CoDE/">project page</a> /
        <!-- <a href="https://aimagelab.github.io/CoDE/">model</a> / -->
        <a href="https://huggingface.co/datasets/elsaEU/ELSA_D3">dataset</a> /
        <a href="references/code.bib">bibtex</a>
        <p>The study proposes CoDE (Contrastive Deepfake Embeddings), a specialized embedding space for 
          effective deepfake detection. By employing a contrastive-learning approach that 
          emphasizes global-local similarities.</p>
      </td>
    </tr>
    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="image\methods_icpr.jpg" alt="code" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://scholar.google.com/citations?view_op=view_citation&hl=it&user=BRG3e1EAAAAJ&citation_for_view=BRG3e1EAAAAJ:zYLM7Y9cAGgC"> 
          <span class="papertitle">Adapt to Scarcity: Few-Shot Deepfake Detection via Low-Rank Adaptation</span>
        </a>
        <br>
        <a>Silvia Cappelletti*</a>,
        <a>Lorenzo Baraldi*</a>,
        <strong>Federico Cocchi*</strong>,
        <a>Marcella Cornia</a>,
        <a>Lorenzo Baraldi</a>,
        <a>Rita Cucchiara</a>,
        <br>
        <em>International Conference on Pattern Recognition (ICPR)</em>, 2024
        <br>
        <a href="https://iris.unimore.it/retrieve/handle/11380/1343567/690594/2024_ICPR_Deepfakes.pdf">paper</a> /  
        <a href="references/icpr.bib">bibtex</a>
        <p>We propose a novel approach using Low-Rank Adaptation (LoRA) of the CLIP architecture, 
          achieving superior performance in few-shot deepfake detection, even with minimal data, across multiple state-of-the-art generators.</p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="image/iciap_plot-1.png" alt="code" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://scholar.google.com/citations?view_op=view_citation&hl=it&user=BRG3e1EAAAAJ&citation_for_view=BRG3e1EAAAAJ:u-x6o8ySG0sC"> 
          <span class="papertitle">Unveiling the Impact of Image Transformations on Deepfake Detection: An Experimental Analysis</span>
        </a>
        <br>
        <strong>Federico Cocchi*</strong>,
        <a>Lorenzo Baraldi*</a>,
        <a>Marcella Cornia</a>,
        <a>Lorenzo Baraldi</a>,
        <a>Rita Cucchiara</a>,
        <br>
        <em>International Conference on Image Analysis ans Processing (ICIAP) - ORAL</em>, 2023
        <br>
        <a href="https://iris.unimore.it/bitstream/11380/1309209/6/2023-iciap-deepfake.pdf">paper</a> /
        <a href="references/iciap_2023.bib">bibtex</a>
        <p>
          This study delves into the critical realm of deepfake detection, emphasizing the importance of countering 
          the potential misuse of generated media for fake news.
          More specifically, we investigate the impact of image transformations on the performance of deepfake detectors,
          considering the most common image manipulations.
        </p>
      </td>
    </tr>
	

    </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Participation to National and European Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="image\logo\elsa_logo.png" alt="code" width="160" style="border-style: none">
              </td>
                <td width="75%" valign="center">
                <a href="https://www.elsa-ai.eu/">ELSA - European Lighthouse on Secure and Safe AI</a>
                <p>
                ELSA is a virtual center of excellence that will spearhead efforts in foundational safe and secure 
                artificial intelligence (AI) methodology research.
                </p>
                <p> I am involved in the Multimedia use cases. Where I develop new approaches to detect deepfake images 
                  to tackle important social challenges such as misinformation. </p>
                <br>

              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="image/logo/fair_project.png" alt="code" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="center">
                <a href="https://fondazione-fair.it/">FAIR - Future Artificial Intelligence Research</a>
                <p>        
                  The FAIR project is a national scale, multidisciplinary initiative aimed at reimagining and developing 
                  large-scale foundational models. It explores research questions, methodologies, models, technologies, 
                  as well as ethical and legal frameworks for creating Artificial Intelligence systems capable of interacting 
                  and collaborating with humans.
                  </p>
                  <p>
                  I am engaged on the trasversal project on vision, language and multimodality.
                  </p>

                <br>

              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="image/logo/itserr_project.png" alt="code" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="center">
                <a href="https://www.itserr.it/">ITSERR</a>
                <p>
                ITSERR is a interdisciplinary and distributed Research Infrastructure for Religious Studies.
                </p>
                <p>
                I am involved in the project as a PhD student, where I work on creation of Large Language Models
                for ancient languages, such as Latin.
                </p>
              
                <br>

              </td>
            </tr>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <h2>Honors & Awards</h2>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="image/ellis_model.png" alt="code" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.ellis.unimore.it/media/project_reports/ELLIS_FLIP_project.pdf"> 
                    <span class="papertitle">Adaptive Patch Selector for Faster Language-Image Pre-training</span>
                  </a>
                  <br>
                  <a>Ayush K Rai*</a>,
                  <a>Bo Wan*</a>,
                  <strong>Federico Cocchi*</strong>,
                  <a>Francesco Tonini*</a>,
                  <a>Giancarlo Paoletti*</a>,
                  <a>Luca Zanella*</a>.
                  <br>
                  <em>ELLIS Summer School, Winner Project</em>, 2023
                  <br>
                  <a href="https://www.ellis.unimore.it/media/project_reports/ELLIS_FLIP_project.pdf">paper</a> 
                  <p>We explore novel methods to enhance masking paradigms such as FLIP, with the objective of accelerating 
                    the pre-training process for a Vision-and-Language model. </p>
                    <!-- Language-Image system -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="poster/s3p_presentation.png" alt="code" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.ellis.unimore.it/media/project_reports/ELLIS_FLIP_project.pdf"> 
                    <span class="papertitle">Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities.</span>
                  </a>
                  <br>
                  <strong>Federico Cocchi</strong>.
                  <br>
                  <em>Summer School on Signal Processing (S3P), <strong>Oral Presentation</strong></em>, 2024.
                  <br>
                  <a href="poster/Poster_CoDE_Summer_School.pdf">poster</a> 
                  <p>Out of over 60 PhD students, my poster was chosen for the plenary oral presentation. </p>
                </td>
              </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Mentorship Activities</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="0"><tbody>

          <tr>
            <td style="padding-left: 20px;"> <!-- Add padding-left here -->
              <ul>
                <li style="margin-bottom: 5px;">Master Thesis Correlator. Luisa Bottiglieri. "Deepfakes Detection: Dividing Real and Fake Images"</li>
                <li style="margin-bottom: 5px;">Master Thesis Correlator. Silvia Cappelletti. "Adapt to Scarcity: Few-Shot Deepfake Detection via Low-Rank Adaptation"</li>
                <li style="margin-bottom: 5px;">Project Tutor of the exam: "Computer Vision and Cognitive Systems"</li>
              </ul>
            </td>
          </tr>  


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Selected Activities as Reviewer</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="0"><tbody>

            <tr>
              <td style="padding-left: 20px;"> <!-- Add padding-left here -->
                <ul>
                  <li style="margin-bottom: 5px;">Computer Vision and Pattern Recognition <b>(CVPR) 2025</b></li>
                  <li style="margin-bottom: 5px;">European Conference on Computer Vision <b>(ECCV) 2024</b></li>
                  <li style="margin-bottom: 5px;">Conference on Computer Vision and Pattern Recognition Workshop <b>(CVPR W) 2024</b> </li>
                  <li style="margin-bottom: 5px;">International Conference on Computer Vision Workshop <b>(ICCV W) 2023</b></li>
                  <li style="margin-bottom: 5px;">International Conference on ACM Multimedia <b>(ACM MM) 2023, 2024</b></li>
                  <li style="margin-bottom: 5px;">Computer Vision and Image Understanding Journal <b>(CVIU) 2024</b></li>
                </ul>
              </td>
            </tr>          
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Website created by Federico | HTML template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>

<!-- 
<tr onmouseout="smerf_stop()" onmouseover="smerf_start()" >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
      <source src="images/smerf.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
      <img src='images/smerf.jpg' width=100%>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://smerf-3d.github.io/">
      <span class="papertitle">prova</span>
    </a>
    <br>
<a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth*</a>,
<a href="https://phogzone.com/">Peter Hedman*</a>,
<a href="https://creiser.github.io/">Christian Reiser</a>,
<a href="">Peter Zhizhin</a>,
<a href="">Jean-François Thibert</a>,
    <a href="https://lucic.ai/">Mario Lučić</a>,
    <a href="https://szeliski.org/">Richard Szeliski</a>,
<strong>Jonathan T. Barron</strong>
    <br>
    <em>arXiv</em>, 2023
    <br>
    <a href="https://smerf-3d.github.io/">project page</a>
    /
    <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
    /
    <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
    <p></p>
    <p>
    Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
    </p>
  </td>
</tr> -->